{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ba947",
   "metadata": {},
   "source": [
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec201886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the topics ahead of time even if we're not sure what the topics are\n",
    "# Each document is represented as a distribution over topics\n",
    "# Each document is presented as a distribution over topics.\n",
    "# Each topic is represented as a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab712bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# Every docuement is a distribution of topics\n",
    "# Every topic is a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1bd30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "\n",
    "# Choose the number of topics you think there are in your corpus\n",
    "# K = 2\n",
    "\n",
    "# Randomly assign each word in each document to one of two topics\n",
    "\n",
    "# Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document\\\n",
    "# and (2) how often the word occurs in the topic overall. Based on this info, assign the word a new topic.\n",
    "\n",
    "# Go thorugh multiple iterations of this. Eventually the topics will start making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5de992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Document-Term Matrix, Number of topics, Number of iterations\n",
    "\n",
    "# Gensim will go through the process of finding the best word distribution\\ \n",
    "# for each topic and best topic distribution for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40312db8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05a95a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the necessary libraries for LDA with Gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06417c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('New_Data.csv', usecols = ['all_features'], low_memory = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d342e590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to string\n",
    "# pre_text = df.squeeze()\n",
    "# text = ' '.join(pre_text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and return a list of tokens\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's wordnet to find the meaning of words, synonoms, antonyms, and more.\n",
    "# Use word lemmatizer to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fucntion that prepares the text for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens\n",
    "# def prepare_text_for_lda(text):\n",
    "#     new_tokens = []\n",
    "#     for token in tokenize(text):\n",
    "#         if (len(token) > 4 and token not in en_stop):\n",
    "#             new_tokens.append(get_lemma(token))\n",
    "#     return new_tokens\n",
    "\n",
    "# retuns a list -  list is full of token \n",
    "def prepare_text_for_lda(text):\n",
    "    return [get_lemma(token) for token in tokenize(text) if (len(token) > 3 and token not in en_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our data read it in line by line. For each line, prepare text for LDA, then add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['21,\"finance', 'good', 'salary', 'nice', 'office', 'work', 'interest', 'vary', 'opportunity', 'travel', 'depend', 'area', 'business', 'people', 'inclusive', 'unsophisticated', 'strategic', 'staff', 'number', 'treat', 'badly', 'staff', 'huge', 'pressure', 'deliver', 'unrealistic', 'deadline', 'dublin', 'corp', 'office', 'hire', 'firing', 'within', 'month', 'probation', 'period', 'pulling', 'people', 'secure', 'job', 'dumping', 'callously', 'management', 'structure', 'place', 'shouting', 'staff', 'distress', 'within', 'month', 'american', 'manager', 'think', 'ireland', 'using', 'practice', 'lawsuit', 'waiting', 'happen']\n['174,\"unclear', 'strategy', 'good', 'salary', 'flexible', 'home', 'working', 'highly', 'centralise', 'decision', 'making']\n['185,\"just', 'innovative', 'interest', 'project', 'poor', 'work', 'life', 'balance']\n['410,\"expert', 'software', 'enginner', 'interest', 'project', 'possibility', 'promotion', 'budget', 'training', 'corporation', 'typical', 'problem', 'encounter', 'higher', 'hierarchy', 'problem', 'financies', 'delay']\n['443,logistics', 'buyer', 'great', 'working', 'environment', 'terms', 'employee', 'mainly', 'salary', 'extremely']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "source": [
    "### LDA with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics you want LDA to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.033*\"salary\" + 0.033*\"interest\" + 0.033*\"work\" + 0.033*\"project\" + 0.033*\"buyer\"')\n(1, '0.012*\"staff\" + 0.012*\"office\" + 0.012*\"within\" + 0.012*\"people\" + 0.011*\"month\"')\n(2, '0.011*\"staff\" + 0.011*\"people\" + 0.011*\"office\" + 0.011*\"month\" + 0.011*\"within\"')\n(3, '0.040*\"good\" + 0.040*\"salary\" + 0.040*\"working\" + 0.040*\"centralise\" + 0.040*\"strategy\"')\n(4, '0.033*\"staff\" + 0.024*\"problem\" + 0.023*\"interest\" + 0.023*\"people\" + 0.023*\"within\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=5)\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "source": [
    "### PYLDAVIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to interpret the topics in a topic model that has been fit to a corpus of text data\n",
    "# Package extracts information from a fitted LDA topic model to inform an interactive web-based visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4772c41da68fc74360513f37641d931d5c41d0cdb820d7cbbc25e68bc1b0486"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}