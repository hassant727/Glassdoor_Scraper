{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ba947",
   "metadata": {},
   "source": [
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec201886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the topics ahead of time even if we're not sure what the topics are\n",
    "# Each document is represented as a distribution over topics\n",
    "# Each document is presented as a distribution over topics.\n",
    "# Each topic is represented as a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab712bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# Every docuement is a distribution of topics\n",
    "# Every topic is a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bd30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "\n",
    "# Choose the number of topics you think there are in your corpus\n",
    "# K = 2\n",
    "\n",
    "# Randomly assign each word in each document to one of two topics\n",
    "\n",
    "# Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document\\\n",
    "# and (2) how often the word occurs in the topic overall. Based on this info, assign the word a new topic.\n",
    "\n",
    "# Go thorugh multiple iterations of this. Eventually the topics will start making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5de992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Document-Term Matrix, Number of topics, Number of iterations\n",
    "\n",
    "# Gensim will go through the process of finding the best word distribution\\ \n",
    "# for each topic and best topic distribution for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40312db8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "05a95a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import the necessary libraries for LDA with Gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06417c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('New_Data.csv', usecols = ['all_features'], low_memory = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d342e590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to string\n",
    "# pre_text = df.squeeze()\n",
    "# text = ' '.join(pre_text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and return a list of tokens\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's wordnet to find the meaning of words, synonoms, antonyms, and more.\n",
    "# Use word lemmatizer to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fucntion that prepares the text for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens\n",
    "# def prepare_text_for_lda(text):\n",
    "#     new_tokens = []\n",
    "#     for token in tokenize(text):\n",
    "#         if (len(token) > 4 and token not in en_stop):\n",
    "#             new_tokens.append(get_lemma(token))\n",
    "#     return new_tokens\n",
    "\n",
    "# retuns a list -  list is full of token \n",
    "def prepare_text_for_lds(text):\n",
    "    return [get_lemma(token) for token in tokenize(text) if (len(token) > 4 and token not in en_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our data read it in line by line. For each line, prepare text for LDA, then add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['14,\"stay', 'clear', 'company', 'people', 'struggle', 'survive', 'toxic', 'environment', 'toxic', 'culture', 'primarily', 'leadership', 'employee', 'value', 'recognition', 'appreciation', 'support', 'disposable', 'widget', 'discard']\n['46,great', 'opportunity', 'stable', 'company', 'benefit', 'growth']\n['57,embedded', 'software', 'engineer', 'environment', 'competitive', 'learn', 'things', 'project', 'assign', 'working', 'hours', 'project', 'assignment']\n['73,neutral', 'globally', 'company', 'master', 'level', 'politics', 'departmental', 'performance', 'rather', 'organization']\n['132,good', 'technology', 'level', 'active', 'safety', 'leadership', 'active', 'safety', 'leadership', 'level', 'strong', 'engineer', 'working', 'level', 'leaders', 'compromise', 'organization']\n['329,\"great', 'place', 'develop', 'personally', 'professionally', 'familiar', 'atmosphere', 'visible', 'hierarchy', 'office', 'collaborative', 'people', 'energy', 'organizational', 'approach', 'management', 'continuous', 'training', 'board', 'possibility', 'develop', 'honestly']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "source": [
    "### LDA with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics you want LDA to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.087*\"people\" + 0.047*\"benefit\" + 0.047*\"interest\" + 0.047*\"holiday\" + 0.047*\"better\"')\n(1, '0.078*\"everything\" + 0.078*\"332,good\" + 0.078*\"balace\" + 0.078*\"going\" + 0.078*\"nothing\"')\n(2, '0.019*\"hire\" + 0.019*\"abreast\" + 0.019*\"match\" + 0.019*\"technology\" + 0.019*\"execution\"')\n(3, '0.069*\"balance\" + 0.069*\"nothing\" + 0.069*\"culture\" + 0.069*\"143,\"best\" + 0.069*\"diversity\"')\n(4, '0.053*\"culture\" + 0.053*\"hire\" + 0.029*\"147,\"aptiv\" + 0.029*\"management\" + 0.029*\"decent\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "source": [
    "### PYLDAVIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to interpret the topics in a topic model that has been fit to a corpus of text data\n",
    "# Package extracts information from a fitted LDA topic model to inform an interactive web-based visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-01eb653bb0a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model5.gensim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlda_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4772c41da68fc74360513f37641d931d5c41d0cdb820d7cbbc25e68bc1b0486"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}