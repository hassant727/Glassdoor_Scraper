{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ba947",
   "metadata": {},
   "source": [
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec201886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the topics ahead of time even if we're not sure what the topics are\n",
    "# Each document is represented as a distribution over topics\n",
    "# Each document is presented as a distribution over topics.\n",
    "# Each topic is represented as a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab712bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# Every docuement is a distribution of topics\n",
    "# Every topic is a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a1bd30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "\n",
    "# Choose the number of topics you think there are in your corpus\n",
    "# K = 2\n",
    "\n",
    "# Randomly assign each word in each document to one of two topics\n",
    "\n",
    "# Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document\\\n",
    "# and (2) how often the word occurs in the topic overall. Based on this info, assign the word a new topic.\n",
    "\n",
    "# Go thorugh multiple iterations of this. Eventually the topics will start making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5de992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Document-Term Matrix, Number of topics, Number of iterations\n",
    "\n",
    "# Gensim will go through the process of finding the best word distribution\\ \n",
    "# for each topic and best topic distribution for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40312db8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "05a95a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the necessary libraries for LDA with Gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06417c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('New_Data.csv', usecols = ['all_features'], low_memory = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d342e590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to string\n",
    "# pre_text = df.squeeze()\n",
    "# text = ' '.join(pre_text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and return a list of tokens\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's wordnet to find the meaning of words, synonoms, antonyms, and more.\n",
    "# Use word lemmatizer to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fucntion that prepares the text for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens\n",
    "# def prepare_text_for_lda(text):\n",
    "#     new_tokens = []\n",
    "#     for token in tokenize(text):\n",
    "#         if (len(token) > 4 and token not in en_stop):\n",
    "#             new_tokens.append(get_lemma(token))\n",
    "#     return new_tokens\n",
    "\n",
    "# retuns a list -  list is full of token \n",
    "def prepare_text_for_lda(text):\n",
    "    return [get_lemma(token) for token in tokenize(text) if (len(token) > 4 and token not in en_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our data read it in line by line. For each line, prepare text for LDA, then add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['55,\"tedious', 'slowly', 'remove', 'every', 'single', 'enthusiasm', 'environment', 'culture', 'balance', 'feedback', 'take', 'seriously', 'management', 'frustrate', 'stack', 'slowly', 'erode', 'enthusiasm', 'might']\n",
      "['66,growing', 'company', 'driving', 'towards', 'limitless', 'learning', 'things', 'notice']\n",
      "['126,\"great', 'place', 'flexibility', 'profesionalism', 'compromise', 'think', 'aways', 'depend', 'business']\n",
      "['164,\"great', 'place', 'great', 'place', 'friendly', 'environment', 'better', 'management', 'support', 'logistics', 'support-', 'computer', 'office', 'phone']\n",
      "['263,worth', 'explore', 'strong', 'vision', 'focus', 'employee', 'safety', 'lack', 'communication', 'expectation']\n",
      "['304,\"great', 'environment', 'people', 'competitive', 'great', 'people', 'benefit', 'really', 'opportunity', 'area', 'interest', 'holiday', 'could', 'better', 'boarding']\n",
      "['358,\"great', 'company', 'environment', 'benefit', 'would', 'recommend', 'work']\n",
      "['408,\"a', 'great', 'place', 'great', 'benefit', 'great', 'worker', 'great', 'communication', 'superior', 'found']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "source": [
    "### LDA with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics you want LDA to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.065*\"slowly\" + 0.065*\"enthusiasm\" + 0.036*\"environment\" + 0.035*\"erode\" + 0.035*\"balance\"')\n(1, '0.014*\"great\" + 0.014*\"place\" + 0.014*\"benefit\" + 0.014*\"environment\" + 0.014*\"company\"')\n(2, '0.050*\"place\" + 0.050*\"flexibility\" + 0.050*\"profesionalism\" + 0.050*\"think\" + 0.050*\"aways\"')\n(3, '0.083*\"great\" + 0.043*\"benefit\" + 0.043*\"communication\" + 0.043*\"company\" + 0.025*\"place\"')\n(4, '0.050*\"place\" + 0.050*\"better\" + 0.050*\"great\" + 0.050*\"environment\" + 0.050*\"people\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=5)\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "source": [
    "### PYLDAVIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to interpret the topics in a topic model that has been fit to a corpus of text data\n",
    "# Package extracts information from a fitted LDA topic model to inform an interactive web-based visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4772c41da68fc74360513f37641d931d5c41d0cdb820d7cbbc25e68bc1b0486"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}