{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ba947",
   "metadata": {},
   "source": [
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec201886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the topics ahead of time even if we're not sure what the topics are\n",
    "# Each document is represented as a distribution over topics\n",
    "# Each document is presented as a distribution over topics.\n",
    "# Each topic is represented as a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab712bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# Every docuement is a distribution of topics\n",
    "# Every topic is a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1bd30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "\n",
    "# Choose the number of topics you think there are in your corpus\n",
    "# K = 2\n",
    "\n",
    "# Randomly assign each word in each document to one of two topics\n",
    "\n",
    "# Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document\\\n",
    "# and (2) how often the word occurs in the topic overall. Based on this info, assign the word a new topic.\n",
    "\n",
    "# Go thorugh multiple iterations of this. Eventually the topics will start making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5de992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Document-Term Matrix, Number of topics, Number of iterations\n",
    "\n",
    "# Gensim will go through the process of finding the best word distribution\\ \n",
    "# for each topic and best topic distribution for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40312db8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05a95a9b",
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\ahmad\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python38\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import the necessary libraries for LDA with Gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06417c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('New_Data.csv', usecols = ['all_features'], low_memory = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d342e590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to string\n",
    "# pre_text = df.squeeze()\n",
    "# text = ' '.join(pre_text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and return a list of tokens\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's wordnet to find the meaning of words, synonoms, antonyms, and more.\n",
    "# Use word lemmatizer to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fucntion that prepares the text for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens\n",
    "# def prepare_text_for_lda(text):\n",
    "#     new_tokens = []\n",
    "#     for token in tokenize(text):\n",
    "#         if (len(token) > 4 and token not in en_stop):\n",
    "#             new_tokens.append(get_lemma(token))\n",
    "#     return new_tokens\n",
    "\n",
    "# retuns a list -  list is full of token \n",
    "def prepare_text_for_lda(text):\n",
    "    return [get_lemma(token) for token in tokenize(text) if (len(token) > 4 and token not in en_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our data read it in line by line. For each line, prepare text for LDA, then add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['139,stock', 'market', 'motivate', 'compensation', 'adequate', 'company', 'company', 'value', 'employee', 'terminate']\n",
      "['146,good', 'benefit', 'benefit', 'company', 'care', 'employee', 'balance', 'overwork']\n",
      "['171,\"working', 'outstanding', 'technology', 'product', 'portfolio', 'really', 'ahead', 'competition', 'quality', 'offer', 'mobility', 'market', 'money', 'costs', 'company', 'executive', 'management', 'willing', 'whatever', 'take', 'people', 'profit', 'disgust', 'disguise', 'think', 'owner', 'guilty', 'working', 'please', 'reply', 'email', 'saturday', 'fire', 'moment', 'accept', 'furlough', 'bother', 'calling', 'working', 'commit', 'great', 'company', 'treat', 'people', 'trash', 'advance', 'company', 'treat', 'garbage', 'value', 'capability', 'space', 'mistake', 'always', 'someone', 'blame', 'course', 'fire']\n",
      "['176,great', 'place', 'fresher', 'automobile', 'industry', 'flexible', 'working', 'hours', 'graduate', 'student', 'ready', 'presentation']\n",
      "['211,\"benefits', 'benefit', 'constant', 'change', 'employee', 'around']\n",
      "['225,\"best', 'company', 'automotive', 'offer', 'package', 'employee', 'place', 'supportive', 'staff', 'nothing', 'mention', 'right']\n",
      "['287,overall', 'place', 'great', 'team', 'great', 'working', 'environment', 'cafeteria', 'suck', 'company', 'holiday']\n",
      "['336,\"technical', 'leader', 'package', 'locate', 'travel']\n",
      "['379,\"so', 'graduate', 'experience', 'salary', 'working', 'hours', 'reepect', 'loyalty']\n",
      "['444,\"worst', 'place', 'work', 'salary', 'future', 'feeling', 'balance']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "source": [
    "### LDA with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics you want LDA to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.050*\"company\" + 0.050*\"place\" + 0.050*\"great\" + 0.027*\"package\" + 0.027*\"offer\"')\n(1, '0.049*\"employee\" + 0.049*\"company\" + 0.027*\"compensation\" + 0.027*\"motivate\" + 0.027*\"terminate\"')\n(2, '0.036*\"salary\" + 0.036*\"balance\" + 0.036*\"444,\"worst\" + 0.036*\"work\" + 0.036*\"future\"')\n(3, '0.054*\"benefit\" + 0.029*\"graduate\" + 0.029*\"hours\" + 0.029*\"working\" + 0.029*\"176,great\"')\n(4, '0.041*\"company\" + 0.028*\"working\" + 0.028*\"treat\" + 0.028*\"fire\" + 0.028*\"people\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "source": [
    "### PYLDAVIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to interpret the topics in a topic model that has been fit to a corpus of text data\n",
    "# Package extracts information from a fitted LDA topic model to inform an interactive web-based visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1-py2.py3-none-any.whl\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-0.24.2-cp38-cp38-win_amd64.whl (6.9 MB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (1.6.3)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0-py2.py3-none-any.whl\n",
      "Collecting funcy\n",
      "  Using cached funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Collecting numexpr\n",
      "  Using cached numexpr-2.7.3-cp38-cp38-win_amd64.whl (93 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (4.0.1)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2-py3-none-any.whl\n",
      "Requirement already satisfied: setuptools in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (57.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pyLDAvis) (1.0.1)\n",
      "Collecting pandas>=1.2.0\n",
      "  Using cached pandas-1.2.4-cp38-cp38-win_amd64.whl (9.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from pandas>=1.2.0->pyLDAvis) (2020.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: Cython==0.29.21 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from gensim->pyLDAvis) (0.29.21)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from gensim->pyLDAvis) (3.0.0)\n",
      "Requirement already satisfied: requests in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from smart-open>=1.8.1->gensim->pyLDAvis) (2.25.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from requests->smart-open>=1.8.1->gensim->pyLDAvis) (2020.12.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\ahmad\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.8_qbz5n2kfra8p0\\localcache\\local-packages\\python38\\site-packages (from scikit-learn->pyLDAvis) (2.1.0)\n",
      "Installing collected packages: scikit-learn, sklearn, pandas, numexpr, future, funcy, pyLDAvis\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\ahmad\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python38\\\\site-packages\\\\sklearn\\\\.libs\\\\vcomp140.dll'\n",
      "Check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-01eb653bb0a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlda\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model5.gensim'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mlda_display\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msort_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mpyLDAvis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlda_display\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis'"
     ]
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4772c41da68fc74360513f37641d931d5c41d0cdb820d7cbbc25e68bc1b0486"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}