{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "661ba947",
   "metadata": {},
   "source": [
    "### The Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ec201886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pick the topics ahead of time even if we're not sure what the topics are\n",
    "# Each document is represented as a distribution over topics\n",
    "# Each document is presented as a distribution over topics.\n",
    "# Each topic is represented as a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ab712bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability Distribution\n",
    "# Every docuement is a distribution of topics\n",
    "# Every topic is a distribution of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a1bd30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: You want LDA to learn the topic mix in each document, and the word mix in each topic\n",
    "\n",
    "# Choose the number of topics you think there are in your corpus\n",
    "# K = 2\n",
    "\n",
    "# Randomly assign each word in each document to one of two topics\n",
    "\n",
    "# Go through every word and its topic assignment in each document. Look at (1) how often the topic occurs in the document\\\n",
    "# and (2) how often the word occurs in the topic overall. Based on this info, assign the word a new topic.\n",
    "\n",
    "# Go thorugh multiple iterations of this. Eventually the topics will start making sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5de992a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input: Document-Term Matrix, Number of topics, Number of iterations\n",
    "\n",
    "# Gensim will go through the process of finding the best word distribution\\ \n",
    "# for each topic and best topic distribution for each document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40312db8",
   "metadata": {},
   "source": [
    "### Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "05a95a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import the necessary libraries for LDA with Gensim\n",
    "from gensim import matutils, models\n",
    "import scipy.sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "06417c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('New_Data.csv', usecols = ['all_features'], low_memory = True)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d342e590",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert columns to string\n",
    "# pre_text = df.squeeze()\n",
    "# text = ' '.join(pre_text)\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3f1f60d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.load('en_core_web_sm')\n",
    "from spacy.lang.en import English\n",
    "parser = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text and return a list of tokens\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's wordnet to find the meaning of words, synonoms, antonyms, and more.\n",
    "# Use word lemmatizer to get the root word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma\n",
    "    \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def get_lemma2(word):\n",
    "    return WordNetLemmatizer().lemmatize(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\ahmad\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a fucntion that prepares the text for topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare_text_for_lda(text):\n",
    "#     tokens = tokenize(text)\n",
    "#     tokens = [token for token in tokens if len(token) > 4]\n",
    "#     tokens = [token for token in tokens if token not in en_stop]\n",
    "#     tokens = [get_lemma(token) for token in tokens]\n",
    "#     return tokens\n",
    "# def prepare_text_for_lda(text):\n",
    "#     new_tokens = []\n",
    "#     for token in tokenize(text):\n",
    "#         if (len(token) > 4 and token not in en_stop):\n",
    "#             new_tokens.append(get_lemma(token))\n",
    "#     return new_tokens\n",
    "\n",
    "# retuns a list -  list is full of token \n",
    "def prepare_text_for_lda(text):\n",
    "    return [get_lemma(token) for token in tokenize(text) if (len(token) > 3 and token not in en_stop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open our data read it in line by line. For each line, prepare text for LDA, then add to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['60,\"greate', 'good', 'great', 'great', 'good', 'great', 'none', 'nope', 'none', 'nope']\n",
      "['325,\"it', 'supply', 'chain', 'manager', 'analyst', 'company', 'good', 'culture', 'values', 'good', 'atmosphere', 'temporary', 'collaborate', 'temporary', 'project']\n",
      "['411,\"good', 'still', 'traditional', 'good', 'people', 'always', 'willing', 'help', 'constant', 'reorgs', 'relatively', 'salary']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "text_data = []\n",
    "with open('dataset.csv', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        tokens = prepare_text_for_lda(line)\n",
    "        if random.random() > .99:\n",
    "            print(tokens)\n",
    "            text_data.append(tokens)"
   ]
  },
  {
   "source": [
    "### LDA with Gensim"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dictionary from the data, then convert to bag-of-words corpus and save the dictionary and corpus for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of topics you want LDA to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(0, '0.036*\"good\" + 0.036*\"nope\" + 0.036*\"great\" + 0.036*\"none\" + 0.036*\"temporary\"')\n(1, '0.036*\"good\" + 0.036*\"great\" + 0.036*\"nope\" + 0.036*\"temporary\" + 0.036*\"60,\"greate\"')\n(2, '0.036*\"good\" + 0.036*\"nope\" + 0.036*\"great\" + 0.036*\"none\" + 0.036*\"temporary\"')\n(3, '0.068*\"help\" + 0.068*\"traditional\" + 0.068*\"reorgs\" + 0.068*\"constant\" + 0.068*\"people\"')\n(4, '0.137*\"good\" + 0.105*\"great\" + 0.072*\"temporary\" + 0.072*\"none\" + 0.072*\"nope\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 5\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(\n",
    "                                                corpus, \n",
    "                                                num_topics = NUM_TOPICS, \n",
    "                                                id2word=dictionary, \n",
    "                                                passes=5\n",
    "                                            )\n",
    "ldamodel.save('model5.gensim')\n",
    "\n",
    "topics = ldamodel.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "\n",
    "#represented as a distribution over words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0, 0.018189339),\n",
       " (1, 0.018189339),\n",
       " (2, 0.018189337),\n",
       " (3, 0.018218162),\n",
       " (4, 0.9272138)]"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "# After configuring model with these parameters, one can test whether it can predict topics for any unseen text document\n",
    "\n",
    "#PROBABILITY SCORES FOR EACH TOPIC IN THE UNSEEN DOCUMENT\n",
    "pred_lda = ldamodel[corpus]\n",
    "pred_lda[0]"
   ]
  },
  {
   "source": [
    "### PYLDAVIS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to interpret the topics in a topic model that has been fit to a corpus of text data\n",
    "# Package extracts information from a fitted LDA topic model to inform an interactive web-based visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "# corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "# lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "\n",
    "# import pyLDAvis.gensim.models\n",
    "# lda_display = pyLDAvis.models.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "# pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0')"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "c4772c41da68fc74360513f37641d931d5c41d0cdb820d7cbbc25e68bc1b0486"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}